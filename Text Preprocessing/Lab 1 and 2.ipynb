{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24ebc1e9",
   "metadata": {},
   "source": [
    "# 1. Lab1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7522bbef",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data = [\n",
    "    \"The movie was fantastic and I loved every part of it about Egypt\",\n",
    "    \"I hated the film, it was the worst I have ever seen\",\n",
    "    \"The storyline was boring but the acting was brilliant\",\n",
    "    \"An amazing movie with a great plot and incredible performances\",\n",
    "    \"Egypt movie, I regret wasting my time on it\",\n",
    "    \"The actors did a great job but the story lacked depth\",\n",
    "    \"One of the best films I have seen in a long time, highly recommend it\",\n",
    "    \"This film was just okay, not too bad but not great either\",\n",
    "    \"Absolutely loved the movie, fantastic plot and wonderful cast\",\n",
    "    \"The movie was disappointing, it did not live up to the hype\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ba5f93",
   "metadata": {},
   "source": [
    "# 1.1.Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "212eff72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['The',\n",
       "  'movie',\n",
       "  'was',\n",
       "  'fantastic',\n",
       "  'and',\n",
       "  'I',\n",
       "  'loved',\n",
       "  'every',\n",
       "  'part',\n",
       "  'of',\n",
       "  'it',\n",
       "  'about',\n",
       "  'Egypt'],\n",
       " ['I',\n",
       "  'hated',\n",
       "  'the',\n",
       "  'film',\n",
       "  ',',\n",
       "  'it',\n",
       "  'was',\n",
       "  'the',\n",
       "  'worst',\n",
       "  'I',\n",
       "  'have',\n",
       "  'ever',\n",
       "  'seen'],\n",
       " ['The',\n",
       "  'storyline',\n",
       "  'was',\n",
       "  'boring',\n",
       "  'but',\n",
       "  'the',\n",
       "  'acting',\n",
       "  'was',\n",
       "  'brilliant'],\n",
       " ['An',\n",
       "  'amazing',\n",
       "  'movie',\n",
       "  'with',\n",
       "  'a',\n",
       "  'great',\n",
       "  'plot',\n",
       "  'and',\n",
       "  'incredible',\n",
       "  'performances'],\n",
       " ['Egypt', 'movie', ',', 'I', 'regret', 'wasting', 'my', 'time', 'on', 'it'],\n",
       " ['The',\n",
       "  'actors',\n",
       "  'did',\n",
       "  'a',\n",
       "  'great',\n",
       "  'job',\n",
       "  'but',\n",
       "  'the',\n",
       "  'story',\n",
       "  'lacked',\n",
       "  'depth'],\n",
       " ['One',\n",
       "  'of',\n",
       "  'the',\n",
       "  'best',\n",
       "  'films',\n",
       "  'I',\n",
       "  'have',\n",
       "  'seen',\n",
       "  'in',\n",
       "  'a',\n",
       "  'long',\n",
       "  'time',\n",
       "  ',',\n",
       "  'highly',\n",
       "  'recommend',\n",
       "  'it'],\n",
       " ['This',\n",
       "  'film',\n",
       "  'was',\n",
       "  'just',\n",
       "  'okay',\n",
       "  ',',\n",
       "  'not',\n",
       "  'too',\n",
       "  'bad',\n",
       "  'but',\n",
       "  'not',\n",
       "  'great',\n",
       "  'either'],\n",
       " ['Absolutely',\n",
       "  'loved',\n",
       "  'the',\n",
       "  'movie',\n",
       "  ',',\n",
       "  'fantastic',\n",
       "  'plot',\n",
       "  'and',\n",
       "  'wonderful',\n",
       "  'cast'],\n",
       " ['The',\n",
       "  'movie',\n",
       "  'was',\n",
       "  'disappointing',\n",
       "  ',',\n",
       "  'it',\n",
       "  'did',\n",
       "  'not',\n",
       "  'live',\n",
       "  'up',\n",
       "  'to',\n",
       "  'the',\n",
       "  'hype']]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "words=[word_tokenize(sent) for sent in text_data]\n",
    "words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8faa40f5",
   "metadata": {},
   "source": [
    "# 1.2. Stopword Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e658294a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " \"he'd\",\n",
       " \"he'll\",\n",
       " \"he's\",\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " \"i'd\",\n",
       " \"i'll\",\n",
       " \"i'm\",\n",
       " \"i've\",\n",
       " 'if',\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it'd\",\n",
       " \"it'll\",\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she'd\",\n",
       " \"she'll\",\n",
       " \"she's\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " \"they'd\",\n",
       " \"they'll\",\n",
       " \"they're\",\n",
       " \"they've\",\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " \"we'd\",\n",
       " \"we'll\",\n",
       " \"we're\",\n",
       " \"we've\",\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words=set(stopwords.words('english'))\n",
    "stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8154afbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['The', 'movie', 'fantastic', 'I', 'loved', 'every', 'part', 'Egypt'],\n",
       " ['I', 'hated', 'film', ',', 'worst', 'I', 'ever', 'seen'],\n",
       " ['The', 'storyline', 'boring', 'acting', 'brilliant'],\n",
       " ['An', 'amazing', 'movie', 'great', 'plot', 'incredible', 'performances'],\n",
       " ['Egypt', 'movie', ',', 'I', 'regret', 'wasting', 'time'],\n",
       " ['The', 'actors', 'great', 'job', 'story', 'lacked', 'depth'],\n",
       " ['One',\n",
       "  'best',\n",
       "  'films',\n",
       "  'I',\n",
       "  'seen',\n",
       "  'long',\n",
       "  'time',\n",
       "  ',',\n",
       "  'highly',\n",
       "  'recommend'],\n",
       " ['This', 'film', 'okay', ',', 'bad', 'great', 'either'],\n",
       " ['Absolutely',\n",
       "  'loved',\n",
       "  'movie',\n",
       "  ',',\n",
       "  'fantastic',\n",
       "  'plot',\n",
       "  'wonderful',\n",
       "  'cast'],\n",
       " ['The', 'movie', 'disappointing', ',', 'live', 'hype']]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_cleaned=[[word for word in sent if word not in stop_words] for sent in words]\n",
    "words_cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53fd6208",
   "metadata": {},
   "source": [
    "# 1.3. Stemming and Lemmatization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4c613948",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['the', 'movy', 'fantast', 'i', 'lov', 'every', 'part', 'egypt'], ['i', 'hat', 'film', ',', 'worst', 'i', 'ev', 'seen'], ['the', 'storylin', 'bor', 'act', 'bril'], ['an', 'amaz', 'movy', 'gre', 'plot', 'incred', 'perform'], ['egypt', 'movy', ',', 'i', 'regret', 'wast', 'tim'], ['the', 'act', 'gre', 'job', 'story', 'lack', 'dep'], ['on', 'best', 'film', 'i', 'seen', 'long', 'tim', ',', 'high', 'recommend'], ['thi', 'film', 'okay', ',', 'bad', 'gre', 'eith'], ['absolv', 'lov', 'movy', ',', 'fantast', 'plot', 'wond', 'cast'], ['the', 'movy', 'disappoint', ',', 'liv', 'hyp']]\n",
      "[['The', 'movie', 'fantastic', 'I', 'loved', 'every', 'part', 'Egypt'], ['I', 'hated', 'film', ',', 'worst', 'I', 'ever', 'seen'], ['The', 'storyline', 'boring', 'acting', 'brilliant'], ['An', 'amazing', 'movie', 'great', 'plot', 'incredible', 'performance'], ['Egypt', 'movie', ',', 'I', 'regret', 'wasting', 'time'], ['The', 'actor', 'great', 'job', 'story', 'lacked', 'depth'], ['One', 'best', 'film', 'I', 'seen', 'long', 'time', ',', 'highly', 'recommend'], ['This', 'film', 'okay', ',', 'bad', 'great', 'either'], ['Absolutely', 'loved', 'movie', ',', 'fantastic', 'plot', 'wonderful', 'cast'], ['The', 'movie', 'disappointing', ',', 'live', 'hype']]\n"
     ]
    }
   ],
   "source": [
    "from nltk import WordNetLemmatizer , LancasterStemmer\n",
    "st=LancasterStemmer()\n",
    "lm=WordNetLemmatizer()\n",
    "stemmed_words=[[st.stem(word) for word in sent] for sent in words_cleaned]\n",
    "lemmatized_words=[[lm.lemmatize(word) for word in sent] for sent in words_cleaned]\n",
    "print(stemmed_words)\n",
    "print(lemmatized_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4870860",
   "metadata": {},
   "source": [
    "# 1.4. Part-of-Speech (POS) Tagging:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "318b5d3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('The', 'DT'),\n",
       "  ('movie', 'NN'),\n",
       "  ('fantastic', 'JJ'),\n",
       "  ('I', 'PRP'),\n",
       "  ('loved', 'VBD'),\n",
       "  ('every', 'DT'),\n",
       "  ('part', 'NN'),\n",
       "  ('Egypt', 'NNP')],\n",
       " [('I', 'PRP'),\n",
       "  ('hated', 'VBD'),\n",
       "  ('film', 'NN'),\n",
       "  (',', ','),\n",
       "  ('worst', 'WP'),\n",
       "  ('I', 'PRP'),\n",
       "  ('ever', 'RB'),\n",
       "  ('seen', 'VBN')],\n",
       " [('The', 'DT'),\n",
       "  ('storyline', 'NN'),\n",
       "  ('boring', 'VBG'),\n",
       "  ('acting', 'VBG'),\n",
       "  ('brilliant', 'NN')],\n",
       " [('An', 'DT'),\n",
       "  ('amazing', 'JJ'),\n",
       "  ('movie', 'NN'),\n",
       "  ('great', 'JJ'),\n",
       "  ('plot', 'NN'),\n",
       "  ('incredible', 'JJ'),\n",
       "  ('performance', 'NN')],\n",
       " [('Egypt', 'NNP'),\n",
       "  ('movie', 'NN'),\n",
       "  (',', ','),\n",
       "  ('I', 'PRP'),\n",
       "  ('regret', 'VBP'),\n",
       "  ('wasting', 'VBG'),\n",
       "  ('time', 'NN')],\n",
       " [('The', 'DT'),\n",
       "  ('actor', 'NN'),\n",
       "  ('great', 'JJ'),\n",
       "  ('job', 'NN'),\n",
       "  ('story', 'NN'),\n",
       "  ('lacked', 'VBD'),\n",
       "  ('depth', 'NN')],\n",
       " [('One', 'CD'),\n",
       "  ('best', 'JJS'),\n",
       "  ('film', 'NN'),\n",
       "  ('I', 'PRP'),\n",
       "  ('seen', 'VBN'),\n",
       "  ('long', 'JJ'),\n",
       "  ('time', 'NN'),\n",
       "  (',', ','),\n",
       "  ('highly', 'RB'),\n",
       "  ('recommend', 'VB')],\n",
       " [('This', 'DT'),\n",
       "  ('film', 'NN'),\n",
       "  ('okay', 'NN'),\n",
       "  (',', ','),\n",
       "  ('bad', 'JJ'),\n",
       "  ('great', 'JJ'),\n",
       "  ('either', 'CC')],\n",
       " [('Absolutely', 'RB'),\n",
       "  ('loved', 'JJ'),\n",
       "  ('movie', 'NN'),\n",
       "  (',', ','),\n",
       "  ('fantastic', 'JJ'),\n",
       "  ('plot', 'NN'),\n",
       "  ('wonderful', 'JJ'),\n",
       "  ('cast', 'NN')],\n",
       " [('The', 'DT'),\n",
       "  ('movie', 'NN'),\n",
       "  ('disappointing', 'NN'),\n",
       "  (',', ','),\n",
       "  ('live', 'VBP'),\n",
       "  ('hype', 'NN')]]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import pos_tag\n",
    "\n",
    "pos_tagged_words=[pos_tag(sent) for sent in lemmatized_words]\n",
    "pos_tagged_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e71ad1",
   "metadata": {},
   "source": [
    "# 1.5. Named Entity Recognition (NER):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e98817bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Tree('S', [('The', 'DT'), ('movie', 'NN'), ('fantastic', 'JJ'), ('I', 'PRP'), ('loved', 'VBD'), ('every', 'DT'), ('part', 'NN'), Tree('PERSON', [('Egypt', 'NNP')])]),\n",
       " Tree('S', [('I', 'PRP'), ('hated', 'VBD'), ('film', 'NN'), (',', ','), ('worst', 'WP'), ('I', 'PRP'), ('ever', 'RB'), ('seen', 'VBN')]),\n",
       " Tree('S', [('The', 'DT'), ('storyline', 'NN'), ('boring', 'VBG'), ('acting', 'VBG'), ('brilliant', 'NN')]),\n",
       " Tree('S', [('An', 'DT'), ('amazing', 'JJ'), ('movie', 'NN'), ('great', 'JJ'), ('plot', 'NN'), ('incredible', 'JJ'), ('performance', 'NN')]),\n",
       " Tree('S', [Tree('GPE', [('Egypt', 'NNP')]), ('movie', 'NN'), (',', ','), ('I', 'PRP'), ('regret', 'VBP'), ('wasting', 'VBG'), ('time', 'NN')]),\n",
       " Tree('S', [('The', 'DT'), ('actor', 'NN'), ('great', 'JJ'), ('job', 'NN'), ('story', 'NN'), ('lacked', 'VBD'), ('depth', 'NN')]),\n",
       " Tree('S', [('One', 'CD'), ('best', 'JJS'), ('film', 'NN'), ('I', 'PRP'), ('seen', 'VBN'), ('long', 'JJ'), ('time', 'NN'), (',', ','), ('highly', 'RB'), ('recommend', 'VB')]),\n",
       " Tree('S', [('This', 'DT'), ('film', 'NN'), ('okay', 'NN'), (',', ','), ('bad', 'JJ'), ('great', 'JJ'), ('either', 'CC')]),\n",
       " Tree('S', [('Absolutely', 'RB'), ('loved', 'JJ'), ('movie', 'NN'), (',', ','), ('fantastic', 'JJ'), ('plot', 'NN'), ('wonderful', 'JJ'), ('cast', 'NN')]),\n",
       " Tree('S', [('The', 'DT'), ('movie', 'NN'), ('disappointing', 'NN'), (',', ','), ('live', 'VBP'), ('hype', 'NN')])]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import ne_chunk\n",
    "\n",
    "ner_words=[ne_chunk(sent) for sent in pos_tagged_words] \n",
    "ner_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a404e0",
   "metadata": {},
   "source": [
    "# 1.6. Saving to a CSV "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cde9b4dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original</th>\n",
       "      <th>filtered</th>\n",
       "      <th>stemmed</th>\n",
       "      <th>lemmatized</th>\n",
       "      <th>POS</th>\n",
       "      <th>NER</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The movie was fantastic and I loved every part...</td>\n",
       "      <td>[The, movie, fantastic, I, loved, every, part,...</td>\n",
       "      <td>[the, movy, fantast, i, lov, every, part, egypt]</td>\n",
       "      <td>[The, movie, fantastic, I, loved, every, part,...</td>\n",
       "      <td>[(The, DT), (movie, NN), (fantastic, JJ), (I, ...</td>\n",
       "      <td>[(The, DT), (movie, NN), (fantastic, JJ), (I, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I hated the film, it was the worst I have ever...</td>\n",
       "      <td>[I, hated, film, ,, worst, I, ever, seen]</td>\n",
       "      <td>[i, hat, film, ,, worst, i, ev, seen]</td>\n",
       "      <td>[I, hated, film, ,, worst, I, ever, seen]</td>\n",
       "      <td>[(I, PRP), (hated, VBD), (film, NN), (,, ,), (...</td>\n",
       "      <td>[(I, PRP), (hated, VBD), (film, NN), (,, ,), (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The storyline was boring but the acting was br...</td>\n",
       "      <td>[The, storyline, boring, acting, brilliant]</td>\n",
       "      <td>[the, storylin, bor, act, bril]</td>\n",
       "      <td>[The, storyline, boring, acting, brilliant]</td>\n",
       "      <td>[(The, DT), (storyline, NN), (boring, VBG), (a...</td>\n",
       "      <td>[(The, DT), (storyline, NN), (boring, VBG), (a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>An amazing movie with a great plot and incredi...</td>\n",
       "      <td>[An, amazing, movie, great, plot, incredible, ...</td>\n",
       "      <td>[an, amaz, movy, gre, plot, incred, perform]</td>\n",
       "      <td>[An, amazing, movie, great, plot, incredible, ...</td>\n",
       "      <td>[(An, DT), (amazing, JJ), (movie, NN), (great,...</td>\n",
       "      <td>[(An, DT), (amazing, JJ), (movie, NN), (great,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Egypt movie, I regret wasting my time on it</td>\n",
       "      <td>[Egypt, movie, ,, I, regret, wasting, time]</td>\n",
       "      <td>[egypt, movy, ,, i, regret, wast, tim]</td>\n",
       "      <td>[Egypt, movie, ,, I, regret, wasting, time]</td>\n",
       "      <td>[(Egypt, NNP), (movie, NN), (,, ,), (I, PRP), ...</td>\n",
       "      <td>[[(Egypt, NNP)], (movie, NN), (,, ,), (I, PRP)...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            original  ...                                                NER\n",
       "0  The movie was fantastic and I loved every part...  ...  [(The, DT), (movie, NN), (fantastic, JJ), (I, ...\n",
       "1  I hated the film, it was the worst I have ever...  ...  [(I, PRP), (hated, VBD), (film, NN), (,, ,), (...\n",
       "2  The storyline was boring but the acting was br...  ...  [(The, DT), (storyline, NN), (boring, VBG), (a...\n",
       "3  An amazing movie with a great plot and incredi...  ...  [(An, DT), (amazing, JJ), (movie, NN), (great,...\n",
       "4        Egypt movie, I regret wasting my time on it  ...  [[(Egypt, NNP)], (movie, NN), (,, ,), (I, PRP)...\n",
       "\n",
       "[5 rows x 6 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df=pd.DataFrame({\n",
    "    \"original\": text_data,\n",
    "    \"filtered\":words_cleaned,\n",
    "    \"stemmed\": stemmed_words,\n",
    "    \"lemmatized\":lemmatized_words,\n",
    "    \"POS\":pos_tagged_words,\n",
    "    \"NER\":ner_words\n",
    "})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9cbb1d5",
   "metadata": {},
   "source": [
    "# 2. Lab2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da279da",
   "metadata": {},
   "source": [
    "## Q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "aeb972cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '''\n",
    "The gross cost of operating lease vehicles in FY2021 Q1 was $4.85 billion.\n",
    "In previous quarter i.e. fy2020 Q4 it was $3 billion fy2020 Q5.\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "011686fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['FY2021 Q1', 'fy2020 Q4']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "pattern=r'\\w{2}\\d{4} \\w[1,4]'\n",
    "re.findall(pattern,text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa5b5c4",
   "metadata": {},
   "source": [
    "## Q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "76b0a303",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '''\n",
    "Follow our leader Elon musk on twitter here: https://twitter.com/elonmusk, more information\n",
    "on Tesla's products can be found at https://www.tesla.com/. Also here are leading influencers\n",
    "for tesla related news,\n",
    "https://twitter.com/teslarati\n",
    "https://twitter.com/dummy_tesla\n",
    "https://twitter.com/dummy_2_tesla\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3b101abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern=r'.com/(\\w+)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e7af929d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['elonmusk', '', 'teslarati', 'dummy_tesla', 'dummy_2_tesla']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(pattern,text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a771d000",
   "metadata": {},
   "source": [
    "## Q3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "606e6eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "text =\"\"\"\n",
    "Hello everyone,  \n",
    "My name is Baraa from the Daturial team. Last week we received many messages from students interested in Artificial Intelligence. Some contacted us via the official email info@daturial.com, while others used the support address support@daturial.org.  \n",
    "One student wrote: “My name is Ahmed, my phone number is 01012345678, and I want to join the upcoming course.” Meanwhile, his colleague Sarah sent her email sarah.ali2025@gmail.com and said she prefers phone communication: 01555555555.  \n",
    "We also received a message from a consulting company using the address contact@ai-solutions.co.uk, which included their website link www.ai-solutions.co.uk. In addition, a university contacted us from admission@university.academy asking about student discounts.  \n",
    "We noticed that some messages only had links such as www.example.com or random numbers like 778899 and a date 2025/09/20. These are not valid emails or phone numbers.  \n",
    "On the other hand, we published an official announcement on our website daturial.com stating that the course begins on 01-10-2025 and ends on 30-11-2025. The course fee is 5000 EGP, and payment can be made via bank transfer to account number 123456789012 or with a Visa card.  \n",
    "Anyone needing more details can email us at baraa.eng@example.com or visit the FAQ page at www.daturial.com/faq.  \n",
    "Finally, we thank everyone who contacted us via email or phone. Please note that any message not signed from domains like daturial.com or daturial.org will not be answered.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3bcddd21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['info@daturial.com', 'ali2025@gmail.com', 'eng@example.com']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "email_pattern=r'\\w+@\\w+.com'\n",
    "re.findall(email_pattern,text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "27bfbc45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['01012345678', '01555555555']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phone_pattern='01\\d{9}'\n",
    "re.findall(phone_pattern,text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5510c861",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nHello everyone,  \\nMy name is Baraa from the Daturial team. Last week we received many messages from students interested in Artificial Intelligence. Some contacted us via the official email [EMAIL HIDDEN], while others used the support address support@daturial.org.  \\nOne student wrote: “My name is Ahmed, my phone number is 01012345678, and I want to join the upcoming course.” Meanwhile, his colleague Sarah sent her email sarah.[EMAIL HIDDEN] and said she prefers phone communication: 01555555555.  \\nWe also received a message from a consulting company using the address contact@ai-solutions.co.uk, which included their website link www.ai-solutions.co.uk. In addition, a university contacted us from admission@university.academy asking about student discounts.  \\nWe noticed that some messages only had links such as www.example.com or random numbers like 778899 and a date 2025/09/20. These are not valid emails or phone numbers.  \\nOn the other hand, we published an official announcement on our website daturial.com stating that the course begins on 01-10-2025 and ends on 30-11-2025. The course fee is 5000 EGP, and payment can be made via bank transfer to account number 123456789012 or with a Visa card.  \\nAnyone needing more details can email us at baraa.[EMAIL HIDDEN] or visit the FAQ page at www.daturial.com/faq.  \\nFinally, we thank everyone who contacted us via email or phone. Please note that any message not signed from domains like daturial.com or daturial.org will not be answered.\\n'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.sub(email_pattern,'[EMAIL HIDDEN]',text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "8a48b2c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['info@daturial.com',\n",
       " 'support@daturial.org',\n",
       " 'sarah.ali2025@gmail.com',\n",
       " 'www.example.com',\n",
       " 'daturial.com',\n",
       " 'baraa.eng@example.com',\n",
       " 'www.daturial.com',\n",
       " 'daturial.com',\n",
       " 'daturial.org']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "URL_patterns=r'(?:www\\.\\w+(?:\\.org|\\.com)|\\S+(?:\\.org|\\.com))'\n",
    "re.findall(URL_patterns,text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab241dbe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
